{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from textwrap import wrap\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'../')\n",
    "\n",
    "import common.datautils as datautils\n",
    "import common.featuresutils as featuresutils\n",
    "import common.flagutils as flagutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_data = datautils.load_ce_results('CE.results.zip')\n",
    "average_data.loc[:, \"Benchmark\"] = average_data[\"Benchmark\"].apply(lambda x: x.split('.')[0])\n",
    "\n",
    "benchmarks = average_data[\"Benchmark\"].unique()\n",
    "features = datautils.load_features(benchmarks)\n",
    "\n",
    "all_flags = flagutils.load_flag_list()\n",
    "\n",
    "average_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = average_data[average_data[\"Flags\"] != '-O3'][[\"Benchmark\", \"Flags\"]]\n",
    "test[\"Benchmark\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(benchmark):\n",
    "    for i in range(len(features)):\n",
    "        if benchmarks[i] == benchmark:\n",
    "            return features[i]\n",
    "        \n",
    "    raise ValueError(\"Features not found for \" + benchmark)\n",
    "    \n",
    "def config_as_labels(config_str):\n",
    "    labels = []\n",
    "    config = config_str[4:].split(' ')\n",
    "    \n",
    "    for i, flag in enumerate(config):\n",
    "        if flag == all_flags[i]:\n",
    "            labels.append(1)  # Flag is turned on\n",
    "        elif flag == '-fno-' + all_flags[i][2:]:\n",
    "            labels.append(0)  # FLag is turned off -fno\n",
    "        else:\n",
    "            raise ValueError(\"ERROR:\" + flag)\n",
    "            \n",
    "    return np.array(labels)\n",
    "\n",
    "def combine_data():\n",
    "    combined = []\n",
    "\n",
    "    for benchmark, config in average_data[average_data[\"Flags\"] != '-O3'][[\"Benchmark\", \"Flags\"]].values:\n",
    "        features = get_features(benchmark)\n",
    "        labels = config_as_labels(config)\n",
    "        \n",
    "        combined.append(labels)\n",
    "        #combined.append(np.concatenate((features, labels)))\n",
    "    \n",
    "    return np.array(combined)\n",
    "\n",
    "def get_target():\n",
    "    o3_values = average_data[average_data[\"Flags\"] == '-O3']\n",
    "    \n",
    "    targets = []\n",
    "    for benchmark, test_energy in average_data[average_data[\"Flags\"] != '-O3'][[\"Benchmark\", \"Energy\"]].values:\n",
    "        o3_energy = o3_values[o3_values[\"Benchmark\"] == benchmark][\"Energy\"].values[0]\n",
    "        \n",
    "        if test_energy < o3_energy:\n",
    "            targets.append('Better')\n",
    "        else:\n",
    "            targets.append('Worse')\n",
    "            \n",
    "    return pd.DataFrame(targets, columns=['Target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = get_target()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.sort_values('Target', ascending=False)\n",
    "test = test.reset_index(drop=True)\n",
    "print(test[test[\"Target\"] == 'Better'].index.min())\n",
    "print(test[test[\"Target\"] == 'Better'].index.max())\n",
    "print(\"\")\n",
    "print(test[test[\"Target\"] == 'Worse'].index.min())\n",
    "print(test[test[\"Target\"] == 'Worse'].index.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = StandardScaler().fit_transform(combine_data())\n",
    "\n",
    "pca = PCA(n_components=5)\n",
    "principal_components = pca.fit_transform(X)\n",
    "component_labels = ['PC1', 'PC2', 'PC3', 'PC4', 'PC5']\n",
    "principal_frame = pd.DataFrame(data=principal_components, columns=component_labels)\n",
    "\n",
    "target = get_target()\n",
    "target = target.sort_values('Target', ascending=False)\n",
    "target = target.reset_index(drop=True)\n",
    "\n",
    "principal_frame = pd.concat([principal_frame, target], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate SCREE Plot\n",
    "fig = plt.figure(figsize=(16,6))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.bar(component_labels, pca.explained_variance_ratio_)\n",
    "ax.set_title('Explained Variance of Principal Components')\n",
    "ax.set_xlabel('Principal Component')\n",
    "ax.set_ylabel('Explained Variance Ratio')\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_title('2 Component PCA plot of stable benchmarks using Milepost features')\n",
    "ax.grid()\n",
    "\n",
    "better_configs = principal_frame[principal_frame[\"Target\"] == 'Better']\n",
    "worse_configs = principal_frame[principal_frame[\"Target\"] == 'Worse']\n",
    "\n",
    "\n",
    "# ax.scatter(better_configs['PC1'], \n",
    "#            better_configs['PC2'], \n",
    "#            label='Better',\n",
    "#            marker='x')\n",
    "\n",
    "# ax.scatter(worse_configs['PC1'], \n",
    "#            worse_configs['PC2'],\n",
    "#           label='Worse')\n",
    "\n",
    "\n",
    "ax.scatter(principal_frame['PC1'], \n",
    "           principal_frame['PC2'])\n",
    "\n",
    "\n",
    "ax.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
