{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from textwrap import wrap\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, LeavePOut\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'../')\n",
    "import common.datautils as datautils\n",
    "import common.featuresutils as featuresutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_data = datautils.load_ce_results('CE.results.zip')\n",
    "ce_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks = ce_data[\"Benchmark\"].unique()\n",
    "benchmarks = np.delete(benchmarks, 16) # Remove LBM\n",
    "print(benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = featuresutils.load_features(benchmarks, with_dwarf=False, with_names=True)\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_apply_data = datautils.load_csv_results(\n",
    "    'cross_apply.results.csv',\n",
    "    ['ReferenceBenchmark','ApplyToBenchmark', 'Success'], \n",
    "    benchmark_cols=['ReferenceBenchmark', 'ApplyToBenchmark'],\n",
    "    successful_only=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_label_data = datautils.load_csv_results('ML.20180904-134133.csv', ['Benchmark', 'Flags', 'Success'])\n",
    "multi_label_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_difference(best_value, measured_value):\n",
    "    difference = (measured_value - best_value) / best_value\n",
    "    return np.round(difference * 100)\n",
    "\n",
    "def calculate_percent_of_best():\n",
    "    for benchmark in benchmarks:\n",
    "        best_config_data = datautils.best_configuration_data('Energy', benchmark, ce_data, ['Energy', 'Time'])\n",
    "\n",
    "        benchmark_ca_data = cross_apply_data[cross_apply_data[\"ApplyToBenchmark\"] == benchmark]\n",
    "        cross_apply_data.loc[benchmark_ca_data.index, 'EnergyPercentOfBest'] = percentage_difference(best_config_data[0], benchmark_ca_data[\"Energy\"])\n",
    "        cross_apply_data.loc[benchmark_ca_data.index, 'TimePercentOfBest'] = percentage_difference(best_config_data[1], benchmark_ca_data[\"Time\"])\n",
    "        \n",
    "\n",
    "calculate_percent_of_best()\n",
    "\n",
    "cross_apply_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(cross_apply_data[\"EnergyPercentOfBest\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentage difference when cross applying best known configurations to other programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_percentage_difference(data):\n",
    "    plt.figure(figsize=(14,14))\n",
    "    \n",
    "    points = np.arange(0, len(benchmarks))\n",
    "    \n",
    "    for i, apply_to_benchmark in enumerate(benchmarks):\n",
    "        for j, reference_benchmark in enumerate(benchmarks):\n",
    "            percentage_difference = data[(data[\"ReferenceBenchmark\"] == reference_benchmark)\n",
    "                                         & (data[\"ApplyToBenchmark\"] == apply_to_benchmark)][\"EnergyPercentOfBest\"].iloc[0]\n",
    "                      \n",
    "            if percentage_difference == -100:\n",
    "                plt.scatter(i, j, marker='_')\n",
    "            else:\n",
    "                if i == j or percentage_difference < 0:\n",
    "                    percentage_difference = 0\n",
    "                    \n",
    "                if percentage_difference > 100:\n",
    "                    percentage_difference = 150\n",
    "                \n",
    "                plt.scatter(i, j, marker='s', c='k', s=percentage_difference)\n",
    "#     plt.scatter(points, points)\n",
    "\n",
    "    plt.title('\\n'.join(wrap(\"Percentage difference in Energy consumption when cross applying the best known configuration for the `Reference` benchmark to `Applied To` benchmark.\", 120)))\n",
    "    plt.xlabel(\"Applied To\")\n",
    "    plt.ylabel(\"Reference\")\n",
    "    plt.xticks(points, benchmarks, rotation='vertical')\n",
    "    plt.yticks(points, benchmarks)\n",
    "    \n",
    "plot_percentage_difference(cross_apply_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN Leave P out Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_value(dict, key, value):\n",
    "    if not key in dict.keys():\n",
    "        dict[key] = []\n",
    "        \n",
    "    dict[key].append(value)\n",
    "    \n",
    "\n",
    "def leave_p_out(p, data):\n",
    "    lpo = LeavePOut(p)\n",
    "    \n",
    "    benchmarks_energy = {}\n",
    "    benchmarks_time = {}\n",
    "    \n",
    "    for train_index, test_index in lpo.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        nn = KNeighborsClassifier(n_neighbors=1)\n",
    "        nn.fit(X_train, y_train)\n",
    "        predictions = nn.predict(X_test)\n",
    "        \n",
    "        for i in range(len(y_test)):\n",
    "            apply_to_benchmark = y_test[i]\n",
    "            predicted_benchmark = predictions[i]\n",
    "            \n",
    "            energy = data[(data[\"ReferenceBenchmark\"] == predicted_benchmark) \n",
    "                          & (data[\"ApplyToBenchmark\"] == apply_to_benchmark)][\"Energy\"].iloc[0]\n",
    "            \n",
    "            time = data[(data[\"ReferenceBenchmark\"] == predicted_benchmark) \n",
    "                          & (data[\"ApplyToBenchmark\"] == apply_to_benchmark)][\"Time\"].iloc[0]\n",
    "        \n",
    "            \n",
    "            add_value(benchmarks_energy, apply_to_benchmark, energy)\n",
    "            add_value(benchmarks_time, apply_to_benchmark, time)\n",
    "\n",
    "    energy_averages = []        \n",
    "    time_averages = []\n",
    "    \n",
    "    for benchmark in benchmarks:\n",
    "        energy_averages.append(np.mean(benchmarks_energy[benchmark]))\n",
    "        time_averages.append(np.mean(benchmarks_time[benchmark]))\n",
    "    \n",
    "    return np.array(energy_averages).reshape(-1, 1), np.array(time_averages).reshape(-1, 1)\n",
    "            \n",
    "    \n",
    "def compare_p(data):\n",
    "    test_p = [1]#,2,3]#,4]\n",
    "    test_p_str = ['LeaveOneOutEnergy', 'LeaveOneOutTime']#, 'LeaveTwoOut', 'LeaveThreeOut'] #'LeaveFourOut']\n",
    "    \n",
    "    energy_averages = []       \n",
    "    time_averages = []\n",
    "    for i in test_p:\n",
    "        p_averages_energy, p_averages_time = leave_p_out(i, data)\n",
    "        \n",
    "        energy_averages.append(p_averages_energy)\n",
    "        time_averages.append(p_averages_time)\n",
    "    \n",
    "    averages = np.concatenate(energy_averages + time_averages, axis=1)\n",
    "    \n",
    "    return pd.DataFrame(averages, index=benchmarks, columns=test_p_str)\n",
    "\n",
    "\n",
    "def cross_o3_relative_data(leave_out_data, average_data, benchmarks):\n",
    "    o3_data = average_data.loc[average_data[\"Flags\"] == \"-O3\"]\n",
    "    \n",
    "    relative_data = leave_out_data.copy(deep=True)\n",
    "    \n",
    "    for benchmark in benchmarks:\n",
    "        o3 = o3_data.loc[average_data[\"Benchmark\"] == benchmark]\n",
    "        o3_energy = o3.iloc[0][\"Energy\"]\n",
    "        o3_time = o3.iloc[0][\"Time\"]\n",
    "\n",
    "        relative_data.loc[benchmark, \"LeaveOneOutEnergy\"] /= o3_energy\n",
    "        relative_data.loc[benchmark, \"LeaveOneOutTime\"] /= o3_time\n",
    "#         relative_data.loc[benchmark, \"LeaveTwoOut\"] /= o3_energy\n",
    "#         relative_data.loc[benchmark, \"LeaveThreeOut\"] /= o3_energy\n",
    "\n",
    "    return relative_data\n",
    "\n",
    "\n",
    "def ce_o3_relative_data(average_data, benchmarks):\n",
    "    o3_data = average_data.loc[average_data[\"Flags\"] == \"-O3\"]\n",
    "    \n",
    "    relative_data = average_data.copy(deep=True)\n",
    "    \n",
    "    for benchmark in benchmarks:\n",
    "        o3 = o3_data.loc[o3_data[\"Benchmark\"] == benchmark]\n",
    "        o3_energy = o3.iloc[0][\"Energy\"]\n",
    "        o3_time = o3.iloc[0][\"Time\"]\n",
    "\n",
    "        relative_data.loc[relative_data[\"Benchmark\"] == benchmark, \"Energy\"] /= o3_energy\n",
    "        relative_data.loc[relative_data[\"Benchmark\"] == benchmark, \"Time\"] /= o3_time\n",
    "\n",
    "    return relative_data\n",
    "\n",
    "def multilabel_o3_relative_data(ce_data, multilabel_data, benchmarks):\n",
    "    o3_data = ce_data.loc[ce_data[\"Flags\"] == \"-O3\"]\n",
    "    \n",
    "    relative_data = multilabel_data.copy(deep=True)\n",
    "    \n",
    "    for benchmark in benchmarks:\n",
    "        o3 = o3_data.loc[o3_data[\"Benchmark\"] == benchmark]\n",
    "        o3_energy = o3.iloc[0][\"Energy\"]\n",
    "        o3_time = o3.iloc[0][\"Time\"]\n",
    "\n",
    "        relative_data.loc[relative_data[\"Benchmark\"] == benchmark, \"Energy\"] /= o3_energy\n",
    "        relative_data.loc[relative_data[\"Benchmark\"] == benchmark, \"Time\"] /= o3_time\n",
    "\n",
    "        \n",
    "    relative_data.loc[:, \"Energy\"] = np.round(relative_data[\"Energy\"], 2)\n",
    "    relative_data.loc[:, \"Time\"] = np.round(relative_data[\"Time\"], 2)\n",
    "    return relative_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_relative_data = ce_o3_relative_data(ce_data, benchmarks)\n",
    "ce_relative_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_relative_data = cross_o3_relative_data(compare_p(cross_apply_data), ce_data, benchmarks)\n",
    "cross_relative_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_relative_data = multilabel_o3_relative_data(ce_data, multi_label_data, benchmarks)\n",
    "multilabel_relative_data = multilabel_relative_data[multilabel_relative_data[\"Benchmark\"] != 'lbm']\n",
    "multilabel_relative_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Improved {len(multilabel_relative_data[multilabel_relative_data['Energy'] <= 1])} / 19 on energy\")\n",
    "print(f\"Average energy reduction for good configs {np.round(1 - np.mean(multilabel_relative_data[multilabel_relative_data['Energy'] < 1]['Energy']) * 100)}\")\n",
    "print(f\"Average energy reduction for all configs {np.round(1 - np.mean(multilabel_relative_data['Energy']))}\")\n",
    "\n",
    "print(f\"Improved {len(multilabel_relative_data[multilabel_relative_data['Time'] <= 1])} / 19 on time\")\n",
    "print(f\"Average time reduction for good configs {np.round(1 - np.mean(multilabel_relative_data[multilabel_relative_data['Time'] < 1]['Time']) * 100)}\")\n",
    "print(f\"Average time reduction for all configs {np.round(1 - np.mean(multilabel_relative_data['Time']))}\")\n",
    "\n",
    "\n",
    "np.mean(multilabel_relative_data['Energy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_relative_to_o3(cross_relative_data, ce_relative_data, ml_relative_data, variable):\n",
    "    X = list(benchmarks)\n",
    "    \n",
    "    #X.remove('lbm') # Something wrong here\n",
    "    \n",
    "    best_known = [ce_relative_data.loc[ce_relative_data[\"Benchmark\"] == benchmark, variable].min() for benchmark in X]\n",
    "    \n",
    "    nn_loo_energy = [cross_relative_data.loc[benchmark, 'LeaveOneOut' + variable] for benchmark in X]\n",
    "#     nn_lto_energy = [cross_relative_data.loc[benchmark, 'LeaveTwoOut'] for benchmark in X]\n",
    "#     nn_ltho_energy = [cross_relative_data.loc[benchmark, 'LeaveThreeOut'] for benchmark in X]\n",
    "    \n",
    "    ml_loo_energy = [ml_relative_data.loc[ml_relative_data[\"Benchmark\"] == benchmark, variable].iloc[0] for benchmark in X]\n",
    "    \n",
    "#     time = [relative_data.loc[relative_data[\"Benchmark\"] == benchmark, 'Time'].min() for benchmark in benchmarks]\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.scatter(X, best_known, label='Best Known')\n",
    "    \n",
    "    plt.scatter(X, nn_loo_energy,  label='1NN (Leave One Out)')\n",
    "#     plt.scatter(X, nn_lto_energy, marker='x', label='1NN (Leave Two Out)')\n",
    "#     plt.scatter(X, nn_ltho_energy, marker='v', label='1NN (Leave Three Out)')\n",
    "\n",
    "    plt.scatter(X, ml_loo_energy, label='CC (Leave One Out)')\n",
    "\n",
    "    plt.axhline(1, label='O3')\n",
    "    \n",
    "    plt.title('Effect of best known config, 1NN predicted config and CC predicted config on ' + variable + ' relative to -O3.')\n",
    "    plt.ylabel('Relative to -O3)')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(np.arange(0.4, 1.6, 0.1))\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    \n",
    "    \n",
    "plot_relative_to_o3(cross_relative_data, ce_relative_data, multilabel_relative_data, 'Energy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_relative_to_o3(cross_relative_data, ce_relative_data, multilabel_relative_data, 'Time')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
