{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile as zip\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from skmultilearn.problem_transform import BinaryRelevance, ClassifierChain, LabelPowerset\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'../')\n",
    "import common.flagutils as flagutils\n",
    "import common.datautils as datautils\n",
    "import common.featuresutils as featuresutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_data = datautils.load_ce_results('CE.results.zip')\n",
    "ce_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3_flags = flagutils.load_o3_flags()\n",
    "all_flags = flagutils.load_flag_list()\n",
    "print(len(all_flags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks = ce_data[\"Benchmark\"].unique()\n",
    "print(benchmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare multi-label classifiers and find optimal paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(with_dwarf):\n",
    "    X = StandardScaler().fit_transform(featuresutils.load_features(benchmarks, with_dwarf=with_dwarf))\n",
    "    y = []\n",
    "\n",
    "    for benchmark in benchmarks:\n",
    "        config_str = datautils.best_configuration('Energy', benchmark, ce_data)\n",
    "        if config_str == '-O3':\n",
    "            config_str = flagutils.get_o3_config()\n",
    "\n",
    "        labels = []\n",
    "\n",
    "        config = config_str[4:].split(' ')\n",
    "        for i, flag in enumerate(config):\n",
    "            if flag == all_flags[i]:\n",
    "                labels.append(1) # Flag is turned on\n",
    "            elif flag == '-fno-' + all_flags[i][2:]:\n",
    "                labels.append(0) # FLag is turned off -fno\n",
    "            else:\n",
    "                print(\"ERROR:\" + flag)\n",
    "\n",
    "        y.append(labels)\n",
    "\n",
    "    return X, np.array(y)\n",
    "\n",
    "\n",
    "def grid_search(classifier, parameters, X, y, n_jobs):\n",
    "    gs = GridSearchCV(classifier, parameters, cv=20, scoring='f1_macro', n_jobs=n_jobs)\n",
    "    gs.fit(X, y)\n",
    "\n",
    "    return gs\n",
    "\n",
    "\n",
    "def test_multilabel_classifier(classifier, X, y, n_jobs):\n",
    "    classifier_name = type(classifier).__name__\n",
    "    \n",
    "    print(f\"Testing {classifier_name}\")\n",
    "    \n",
    "    parameters = [\n",
    "        {\n",
    "            'classifier': [SVC(kernel='linear')],\n",
    "            'classifier__C': np.logspace(-2, 10, 13)\n",
    "        },\n",
    "        {\n",
    "            'classifier': [SVC(kernel='rbf')],\n",
    "            'classifier__C': np.logspace(-2, 10, 13), \n",
    "            'classifier__gamma':  np.logspace(-9, 3, 13)\n",
    "        }\n",
    "#         {\n",
    "#             'classifier': [GaussianNB(), DecisionTreeClassifier(max_depth=5), AdaBoostClassifier(), DecisionTreeClassifier(max_depth=5)]\n",
    "#         }\n",
    "    ]\n",
    "    gs = grid_search(classifier, parameters, X, y, n_jobs)\n",
    "\n",
    "    print(f\"{classifier_name} Best F1 Score: {gs.best_score_}\")\n",
    "    print(gs.best_params_)\n",
    "    print(\"---------\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_comparison(multilabel_classifiers, n_jobs, disable_warnings=False, with_dwarf=False):\n",
    "    X, y = load_data(with_dwarf)\n",
    "    y = flagutils.remove_static_labels(y)\n",
    "\n",
    "    print(f\"Loaded {X.shape[0]} samples with {X.shape[1]} features and {y.shape[1]} labels.\")\n",
    "    \n",
    "#     Grid search can throw a lot of warnings. Is useful to just mute them sometimes.\n",
    "    if disable_warnings:\n",
    "        import warnings\n",
    "        warnings.filterwarnings('ignore')\n",
    "        \n",
    "    for clf in multilabel_classifiers:\n",
    "        test_multilabel_classifier(clf, X, y, n_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_comparison([BinaryRelevance(), ClassifierChain()], -1, disable_warnings=True, with_dwarf=False)\n",
    "\n",
    "# Loaded 20 samples with 65 features and 94 labels.\n",
    "# Testing BinaryRelevance\n",
    "# BinaryRelevance Best F1 Score: 0.5882978723404256\n",
    "# {'classifier': SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "#   decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
    "#   max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "#   tol=0.001, verbose=False), 'classifier__C': 1.0, 'classifier__gamma': 0.1}\n",
    "# ---------\n",
    "# Testing ClassifierChain\n",
    "# ClassifierChain Best F1 Score: 0.625531914893617\n",
    "# {'classifier': SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "#   decision_function_shape='ovr', degree=3, gamma=0.01, kernel='rbf',\n",
    "#   max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "#   tol=0.001, verbose=False), 'classifier__C': 1.0, 'classifier__gamma': 0.01}\n",
    "# ---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_comparison([BinaryRelevance(), ClassifierChain(), LabelPowerset()], -1, disable_warnings=True, with_dwarf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_o3_config():\n",
    "    X, y_all = load_data(False)\n",
    "    y_subset, remaining_labels = flagutils.remove_static_labels(y_all, with_labels=True)\n",
    "\n",
    "    full_o3_config = flagutils.get_o3_config().split(' ')\n",
    "    removed_labels = np.setdiff1d(all_flags, remaining_labels)\n",
    "\n",
    "    \n",
    "    return flagutils.get_cmd_string_from_config([f for f in full_o3_config if f not in removed_labels])\n",
    "\n",
    "min_o3_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_in_o3(with_dwarf=False):\n",
    "    X, y_all = load_data(with_dwarf)\n",
    "    y_subset, remaining_labels = flagutils.remove_static_labels(y_all, with_labels=True)\n",
    "    \n",
    "    print(y_all.shape)\n",
    "    \n",
    "    removed_labels = np.setdiff1d(all_flags, remaining_labels)\n",
    "    \n",
    "    return [f for f in removed_labels if f not in o3_flags]\n",
    "\n",
    "not_in_o3() \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output predicted configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_predicted_configs():\n",
    "    X, y = datautils.format_data_for_multilabel(ce_data, False, benchmarks)\n",
    "    y, remaining_labels = flagutils.remove_static_labels(y, with_labels=True)\n",
    "\n",
    "    kf = KFold(n_splits=len(y))\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        clf = ClassifierChain(SVC(kernel='rbf', C=0.01, gamma=0.01))\n",
    "        clf.fit(X_train, y_train)\n",
    "        prediction = clf.predict(X_test).toarray()\n",
    "\n",
    "        config = \"\"\n",
    "\n",
    "        for index, flag_enabled in enumerate(prediction[0]):\n",
    "            if flag_enabled:\n",
    "                config += remaining_labels[0][index] + \" \"\n",
    "            else:\n",
    "                config += \"-fno-\" + remaining_labels[0][index][2:] + \" \"\n",
    "\n",
    "        print(benchmarks[test_index])\n",
    "        print(config)\n",
    "        print(\"-----\")\n",
    "\n",
    "# print_predicted_configs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier chain results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_label_data = datautils.load_csv_results('ML.20180904-134133.csv', ['Benchmark', 'Flags', 'Success'])\n",
    "multi_label_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ric_data = datautils.load_zip_results('RIC.results.zip', ['Benchmark','Flags', 'RunId', 'Success'])\n",
    "ric_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def o3_relative_data(ce_data, ml_data):\n",
    "    o3_data = ce_data.loc[ce_data[\"Flags\"] == \"-O3\"]\n",
    "    \n",
    "    relative_data = ml_data.copy(deep=True)\n",
    "    \n",
    "    for benchmark in ml_data[\"Benchmark\"].unique():\n",
    "        o3 = o3_data.loc[o3_data[\"Benchmark\"] == benchmark]\n",
    "        o3_energy = o3.iloc[0][\"Energy\"]\n",
    "        o3_time = o3.iloc[0][\"Time\"]\n",
    "\n",
    "        relative_data.loc[relative_data[\"Benchmark\"] == benchmark, \"Energy\"] /= o3_energy\n",
    "        relative_data.loc[relative_data[\"Benchmark\"] == benchmark, \"Time\"] /= o3_time\n",
    "\n",
    "    return relative_data[relative_data[\"Flags\"] != '-O3']\n",
    "\n",
    "def compute_probability(reduction):\n",
    "    \n",
    "    total_configs = 0\n",
    "    good_configs = 0\n",
    "    \n",
    "    for benchmark in ric_data[\"Benchmark\"].unique():\n",
    "        benchmark_data = ric_data[ric_data[\"Benchmark\"] == benchmark]\n",
    "        \n",
    "        o3_data = benchmark_data[benchmark_data[\"Flags\"] == '-O3']\n",
    "        o3_energy = o3_data[\"Energy\"].iloc[0]\n",
    "        \n",
    "        total_configs += len(benchmark_data) - 1 # Don't count O3\n",
    "        good_configs += len(benchmark_data[benchmark_data[\"Energy\"] < (o3_energy * reduction)])\n",
    "    \n",
    "    good_config_probability = good_configs / total_configs\n",
    "    \n",
    "    return good_config_probability\n",
    "\n",
    "def improvement_significance(significance_level):\n",
    "    sig_relative_data = o3_relative_data(ce_data, multi_label_data)\n",
    "\n",
    "    columns = [\"Improvement\", \"P-Value\", \"Significant\"]\n",
    "    benchmarks_with_improvement = []\n",
    "    results = []\n",
    "    \n",
    "    for benchmark in sig_relative_data[\"Benchmark\"].unique():\n",
    "        benchmark_data = sig_relative_data[sig_relative_data[\"Benchmark\"] == benchmark]\n",
    "        if benchmark_data[\"Energy\"].iloc[0] < 1:\n",
    "            benchmarks_with_improvement.append(benchmark)\n",
    "            probability = np.round(compute_probability(benchmark_data[\"Energy\"].iloc[0]), 2)\n",
    "            improvement = np.round(1 - benchmark_data[\"Energy\"].iloc[0], 2) * 100\n",
    "            \n",
    "            if probability < significance_level:\n",
    "                results.append([improvement,probability,'Yes'])\n",
    "            else:\n",
    "                results.append([improvement,probability,'No'])\n",
    "                \n",
    "    return pd.DataFrame(results, columns=columns, index=benchmarks_with_improvement).sort_values('Improvement', ascending=False)\n",
    "\n",
    "len(improvement_significance(0.05))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
