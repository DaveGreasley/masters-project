{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile as zip\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from skmultilearn.problem_transform import BinaryRelevance, ClassifierChain, LabelPowerset\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'../')\n",
    "from common.datautils import load_ce_results\n",
    "from common.datautils import format_data_for_multilabel\n",
    "from common.featuresutils import load_features\n",
    "from common.flagutils import load_flag_list\n",
    "from common.flagutils import load_o3_flags\n",
    "from common.flagutils import build_config\n",
    "from common.flagutils import get_cmd_string_from_config\n",
    "from common.flagutils import remove_static_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_data = load_ce_results('CE.results.zip')\n",
    "average_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_flags = load_flag_list()\n",
    "print(len(all_flags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_configuration(variable, benchmark, average_data):\n",
    "    benchmark_data = average_data.loc[average_data[\"Benchmark\"] == benchmark]\n",
    "    min_index = benchmark_data[variable].idxmin()\n",
    "    return benchmark_data.loc[min_index][\"Flags\"]\n",
    "\n",
    "\n",
    "def get_o3_config():\n",
    "    return get_cmd_string_from_config(build_config(all_flags, load_o3_flags(), '-O3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks = average_data[\"Benchmark\"].unique()\n",
    "benchmarks_nosize = [b.split('.')[0] for b in benchmarks]\n",
    "print(benchmarks_nosize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(with_dwarf):\n",
    "    X = StandardScaler().fit_transform(load_features(benchmarks_nosize, with_dwarf=with_dwarf))\n",
    "    y = []\n",
    "\n",
    "    for benchmark in benchmarks:\n",
    "        config_str = best_configuration('Energy', benchmark, average_data)\n",
    "        if config_str == '-O3':\n",
    "            config_str = get_o3_config()\n",
    "\n",
    "        labels = []\n",
    "\n",
    "        config = config_str[4:].split(' ')\n",
    "        for i, flag in enumerate(config):\n",
    "            if flag == all_flags[i]:\n",
    "                labels.append(1) # Flag is turned on\n",
    "            elif flag == '-fno-' + all_flags[i][2:]:\n",
    "                labels.append(0) # FLag is turned off -fno\n",
    "            else:\n",
    "                print(\"ERROR:\" + flag)\n",
    "\n",
    "        y.append(labels)\n",
    "\n",
    "    return X, np.array(y)\n",
    "\n",
    "\n",
    "def grid_search(classifier, parameters, X, y):\n",
    "    gs = GridSearchCV(classifier, parameters, cv=15, scoring='f1_macro')\n",
    "    gs.fit(X, y)\n",
    "\n",
    "    return gs\n",
    "\n",
    "\n",
    "def test_multilabel_classifier(classifier, X, y):\n",
    "    classifier_name = type(classifier).__name__\n",
    "    \n",
    "    print(f\"Testing {classifier_name}\")\n",
    "    \n",
    "    parameters = [\n",
    "        {\n",
    "            'classifier': [SVC(kernel='linear')],\n",
    "            'classifier__C': np.logspace(-2, 10, 13)\n",
    "        },\n",
    "        {\n",
    "            'classifier': [SVC(kernel='rbf')],\n",
    "            'classifier__C': np.logspace(-2, 10, 13), \n",
    "            'classifier__gamma':  np.logspace(-9, 3, 13)\n",
    "        },\n",
    "        {\n",
    "            'classifier': [GaussianNB(), DecisionTreeClassifier(max_depth=5), AdaBoostClassifier(), DecisionTreeClassifier(max_depth=5)]\n",
    "        }\n",
    "    ]\n",
    "    gs = grid_search(classifier, parameters, X, y)\n",
    "\n",
    "    print(f\"{classifier_name} Best F1 Score: {gs.best_score_}\")\n",
    "    print(gs.best_params_)\n",
    "    print(\"---------\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_comparison(multilabel_classifiers, disable_warnings=False, with_dwarf=False):\n",
    "    X, y = load_data(with_dwarf)\n",
    "    y = remove_static_labels(y)\n",
    "\n",
    "    print(f\"Loaded {X.shape[0]} samples with {X.shape[1]} features and {y.shape[1]} labels.\")\n",
    "    \n",
    "    # Grid search can throw a lot of warnings. Is useful to just mute them sometimes.\n",
    "#     if disable_warnings:\n",
    "#         import warnings\n",
    "#         warnings.filterwarnings('ignore')\n",
    "        \n",
    "    for clf in multilabel_classifiers:\n",
    "        test_multilabel_classifier(clf, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_comparison([BinaryRelevance(), ClassifierChain(), LabelPowerset()], disable_warnings=True, with_dwarf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_comparison([BinaryRelevance(), ClassifierChain(), LabelPowerset()], disable_warnings=True, with_dwarf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = format_data_for_multilabel(average_data, False, benchmarks)\n",
    "y, remaining_labels = remove_static_labels(y, with_labels=True)\n",
    "\n",
    "kf = KFold(n_splits=len(y))\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    clf = ClassifierChain(SVC(kernel='rbf', C=0.01, gamma=0.01))\n",
    "    clf.fit(X_train, y_train)\n",
    "    prediction = clf.predict(X_test).toarray()\n",
    "\n",
    "    config = \"\"\n",
    "\n",
    "    for index, flag_enabled in enumerate(prediction[0]):\n",
    "        if flag_enabled:\n",
    "            config += remaining_labels[0][index] + \" \"\n",
    "        else:\n",
    "            config += \"-fno-\" + remaining_labels[0][index] + \" \"\n",
    "\n",
    "    print(benchmarks[test_index])\n",
    "    print(config)\n",
    "    print(\"-----\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
